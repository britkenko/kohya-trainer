{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/britkenko/kohya-trainer/blob/main/240418_sdxl_LoRA_trainer_XL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slgjeYgd6pWp"
      },
      "source": [
        "[![visitor][visitor-badge]][visitor-stats]\n",
        "[![ko-fi][ko-fi-badge]][ko-fi-link]\n",
        "\n",
        "# **Kohya LoRA Trainer XL**\n",
        "A Colab Notebook For SDXL LoRA Training (Fine-tuning Method)\n",
        "*Johnsons Fork*\n",
        "\n",
        "[visitor-badge]: https://api.visitorbadge.io/api/visitors?path=Kohya%20LoRA%20Trainer%20XL&label=Visitors&labelColor=%2334495E&countColor=%231ABC9C&style=flat&labelStyle=none\n",
        "[visitor-stats]: https://visitorbadge.io/status?path=Kohya%20LoRA%20Trainer%20XL\n",
        "[ko-fi-badge]: https://img.shields.io/badge/Support%20me%20on%20Ko--fi-F16061?logo=ko-fi&logoColor=white&style=flat\n",
        "[ko-fi-link]: https://ko-fi.com/linaqruf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MxF9feWshAp"
      },
      "source": [
        "| Notebook Name | Description | Link |\n",
        "| --- | --- | --- |\n",
        "| [Kohya LoRA Trainer XL](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-trainer-XL.ipynb) | LoRA Training | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=flat)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-trainer-XL.ipynb) |\n",
        "| [Kohya Trainer XL](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-trainer-XL.ipynb) | Native Training | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=flat)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-trainer-XL.ipynb) |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTVqCAgSmie4"
      },
      "source": [
        "# **I. Prepare Environment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6uDIVmL03SC",
        "outputId": "96b7800e-4b64-4cd1-a052-cfced41aca90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Apr 18 00:16:00 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0              41W / 350W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# You can run this notebook in Tesla T4 but make sure you're using High RAM setting. (Colab Pro gated)\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_u3q60di584x",
        "outputId": "45fe4237-cab8-4274-a85f-1e07b8a7bac5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: torch 2.2.2\n",
            "Uninstalling torch-2.2.2:\n",
            "  Successfully uninstalled torch-2.2.2\n",
            "Found existing installation: torchaudio 2.2.2\n",
            "Uninstalling torchaudio-2.2.2:\n",
            "  Successfully uninstalled torchaudio-2.2.2\n",
            "Found existing installation: torchvision 0.17.2\n",
            "Uninstalling torchvision-0.17.2:\n",
            "  Successfully uninstalled torchvision-0.17.2\n",
            "Found existing installation: torchtext 0.17.2\n",
            "Uninstalling torchtext-0.17.2:\n",
            "  Successfully uninstalled torchtext-0.17.2\n",
            "Found existing installation: torchdata 0.7.1\n",
            "Uninstalling torchdata-0.7.1:\n",
            "  Successfully uninstalled torchdata-0.7.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cufft-cu12 (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting torch\n",
            "  Using cached torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchaudio\n",
            "  Using cached torchaudio-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.17.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting torchtext\n",
            "  Using cached torchtext-0.17.2-cp310-cp310-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Collecting torchdata\n",
            "  Using cached torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Using cached torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached torchaudio-2.2.2-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n",
            "Using cached torchvision-0.17.2-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
            "Using cached torchtext-0.17.2-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "Using cached torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cufft-cu12 (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: nvidia-cusparse-cu12, nvidia-cufft-cu12, torch, torchvision, torchtext, torchdata, torchaudio\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n",
            "    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n",
            "Successfully installed nvidia-cufft-cu12-11.0.2.54 nvidia-cusparse-cu12-12.1.0.106 torch-2.2.2 torchaudio-2.2.2 torchdata-0.7.1 torchtext-0.17.2 torchvision-0.17.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: xformers in /usr/local/lib/python3.10/dist-packages (0.0.25.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.25.2)\n",
            "Requirement already satisfied: torch==2.2.2 in /usr/local/lib/python3.10/dist-packages (from xformers) (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->xformers) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->xformers) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->xformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->xformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->xformers) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->xformers) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->xformers) (12.1.105)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.2->xformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.2->xformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->xformers) (8.9.2.26)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.2->xformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->xformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->xformers) (10.3.2.106)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.2->xformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->xformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->xformers) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->xformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->xformers) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2->xformers) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.2->xformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.2->xformers) (1.3.0)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Installing collected packages: nvidia-cuda-runtime-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n",
            "    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cusolver-cu12-11.4.5.107\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
            "Requirement already satisfied: jax==0.4.23 in /usr/local/lib/python3.10/dist-packages (from jax[cuda12_pip]==0.4.23) (0.4.23)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.23->jax[cuda12_pip]==0.4.23) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.23->jax[cuda12_pip]==0.4.23) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax==0.4.23->jax[cuda12_pip]==0.4.23) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.23->jax[cuda12_pip]==0.4.23) (1.11.4)\n",
            "\u001b[33mWARNING: jax 0.4.23 does not provide the extra 'cuda12-pip'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: jaxlib==0.4.23+cuda12.cudnn89 in /usr/local/lib/python3.10/dist-packages (from jax[cuda12_pip]==0.4.23) (0.4.23+cuda12.cudnn89)\n",
            "Collecting nvidia-cublas-cu12>=12.2.5.6 (from jax[cuda12_pip]==0.4.23)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12>=12.2.142 (from jax[cuda12_pip]==0.4.23)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cuda-nvcc-cu12>=12.2.140 in /usr/local/lib/python3.10/dist-packages (from jax[cuda12_pip]==0.4.23) (12.4.131)\n",
            "Collecting nvidia-cuda-runtime-cu12>=12.2.140 (from jax[cuda12_pip]==0.4.23)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12>=8.9 in /usr/local/lib/python3.10/dist-packages (from jax[cuda12_pip]==0.4.23) (8.9.2.26)\n",
            "Collecting nvidia-cufft-cu12>=11.0.8.103 (from jax[cuda12_pip]==0.4.23)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12>=11.5.2 (from jax[cuda12_pip]==0.4.23)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12>=12.1.2.141 (from jax[cuda12_pip]==0.4.23)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12>=2.18.3 in /usr/local/lib/python3.10/dist-packages (from jax[cuda12_pip]==0.4.23) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12>=12.2 in /usr/local/lib/python3.10/dist-packages (from jax[cuda12_pip]==0.4.23) (12.4.127)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "Installing collected packages: nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n",
            "    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n",
            "    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n",
            "    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.2.2 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.4.5.8 which is incompatible.\n",
            "torch 2.2.2 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.4.127 which is incompatible.\n",
            "torch 2.2.2 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.4.127 which is incompatible.\n",
            "torch 2.2.2 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.1.3 which is incompatible.\n",
            "torch 2.2.2 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.1.9 which is incompatible.\n",
            "torch 2.2.2 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.3.1.170 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cufft-cu12-11.2.1.3 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: scvi-tools in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Requirement already satisfied: anndata>=0.7.5 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (0.10.7)\n",
            "Requirement already satisfied: docrep>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (0.3.2)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (0.8.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (3.9.0)\n",
            "Requirement already satisfied: jax>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (0.4.23)\n",
            "Requirement already satisfied: jaxlib>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (0.4.23+cuda12.cudnn89)\n",
            "Requirement already satisfied: lightning<2.2,>=2.0 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (2.1.4)\n",
            "Requirement already satisfied: ml-collections>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (0.1.1)\n",
            "Requirement already satisfied: mudata>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (0.2.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (1.25.2)\n",
            "Requirement already satisfied: numpyro>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (0.14.0)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (0.2.2)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (2.0.3)\n",
            "Requirement already satisfied: pyro-ppl>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (1.9.0)\n",
            "Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (13.7.1)\n",
            "Requirement already satisfied: scikit-learn>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (2.2.2)\n",
            "Requirement already satisfied: torchmetrics>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (1.3.2)\n",
            "Requirement already satisfied: tqdm>=4.56.0 in /usr/local/lib/python3.10/dist-packages (from scvi-tools) (4.66.2)\n",
            "Requirement already satisfied: array-api-compat!=1.5,>1.4 in /usr/local/lib/python3.10/dist-packages (from anndata>=0.7.5->scvi-tools) (1.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anndata>=0.7.5->scvi-tools) (1.2.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from anndata>=0.7.5->scvi-tools) (8.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from anndata>=0.7.5->scvi-tools) (24.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from docrep>=0.3.2->scvi-tools) (1.16.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.4->scvi-tools) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.4->scvi-tools) (3.3.0)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning<2.2,>=2.0->scvi-tools) (6.0.1)\n",
            "Requirement already satisfied: fsspec<2025.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->scvi-tools) (2023.6.0)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from lightning<2.2,>=2.0->scvi-tools) (0.11.2)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from lightning<2.2,>=2.0->scvi-tools) (4.11.0)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning<2.2,>=2.0->scvi-tools) (1.9.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from ml-collections>=0.1.1->scvi-tools) (1.4.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from ml-collections>=0.1.1->scvi-tools) (21.6.0)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.10/dist-packages (from numpyro>=0.12.1->scvi-tools) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->scvi-tools) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->scvi-tools) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->scvi-tools) (2024.1)\n",
            "Requirement already satisfied: pyro-api>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl>=1.6.0->scvi-tools) (0.1.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->scvi-tools) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->scvi-tools) (2.16.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.2->scvi-tools) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.2->scvi-tools) (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->scvi-tools) (3.13.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->scvi-tools) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->scvi-tools) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->scvi-tools) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->scvi-tools) (12.1.105)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->scvi-tools)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->scvi-tools)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->scvi-tools) (8.9.2.26)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->scvi-tools)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->scvi-tools)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->scvi-tools) (10.3.2.106)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->scvi-tools)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->scvi-tools)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->scvi-tools) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->scvi-tools) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->scvi-tools) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->scvi-tools) (12.4.127)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax->scvi-tools) (1.0.8)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax->scvi-tools) (0.4.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax->scvi-tools) (0.1.45)\n",
            "Requirement already satisfied: chex>=0.1.86 in /usr/local/lib/python3.10/dist-packages (from optax->scvi-tools) (0.1.86)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.86->optax->scvi-tools) (0.12.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->scvi-tools) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->scvi-tools) (3.9.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.8.0->lightning<2.2,>=2.0->scvi-tools) (67.7.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->scvi-tools) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->scvi-tools) (2.1.5)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax->scvi-tools) (1.7.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax->scvi-tools) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax->scvi-tools) (3.20.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->scvi-tools) (1.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->scvi-tools) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->scvi-tools) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->scvi-tools) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->scvi-tools) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->scvi-tools) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->scvi-tools) (4.0.3)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax->scvi-tools) (6.4.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax->scvi-tools) (3.18.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->scvi-tools) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->scvi-tools) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->scvi-tools) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->scvi-tools) (2024.2.2)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Installing collected packages: nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n",
            "    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n",
            "    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cufft-cu12-11.0.2.54 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "lz4 is already the newest version (1.9.3-2build2).\n",
            "aria2 is already the newest version (1.36.0-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip3 uninstall --yes torch torchaudio torchvision torchtext torchdata\n",
        "!pip3 install torch torchaudio torchvision torchtext torchdata\n",
        "!pip install xformers\n",
        "!pip install \"jax[cuda12_pip]==0.4.23\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "!pip install scvi-tools\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "import time\n",
        "import requests\n",
        "import torch\n",
        "from subprocess import getoutput\n",
        "from IPython.utils import capture\n",
        "from google.colab import drive\n",
        "\n",
        "%store -r\n",
        "\n",
        "# root_dir\n",
        "root_dir          = \"/content\"\n",
        "drive_dir         = os.path.join(root_dir, \"drive/MyDrive\")\n",
        "deps_dir          = os.path.join(root_dir, \"deps\")\n",
        "repo_dir          = os.path.join(root_dir, \"kohya-trainer\")\n",
        "training_dir      = os.path.join(root_dir, \"LoRA\")\n",
        "pretrained_model  = os.path.join(root_dir, \"pretrained_model\")\n",
        "vae_dir           = os.path.join(root_dir, \"vae\")\n",
        "lora_dir          = os.path.join(root_dir, \"network_weight\")\n",
        "repositories_dir  = os.path.join(root_dir, \"repositories\")\n",
        "config_dir        = os.path.join(training_dir, \"config\")\n",
        "tools_dir         = os.path.join(repo_dir, \"tools\")\n",
        "finetune_dir      = os.path.join(repo_dir, \"finetune\")\n",
        "accelerate_config = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "\n",
        "for store in [\"root_dir\", \"repo_dir\", \"training_dir\", \"pretrained_model\", \"vae_dir\", \"repositories_dir\", \"accelerate_config\", \"tools_dir\", \"finetune_dir\", \"config_dir\"]:\n",
        "    with capture.capture_output() as cap:\n",
        "        %store {store}\n",
        "        del cap\n",
        "\n",
        "repo_dict = {\n",
        "    \"qaneel/kohya-trainer (forked repo, stable, optimized for colab use)\" : \"https://github.com/qaneel/kohya-trainer\",\n",
        "    \"kohya-ss/sd-scripts (original repo, latest update)\"                    : \"https://github.com/kohya-ss/sd-scripts\",\n",
        "}\n",
        "\n",
        "repository        = \"qaneel/kohya-trainer (forked repo, stable, optimized for colab use)\" #@param [\"qaneel/kohya-trainer (forked repo, stable, optimized for colab use)\", \"kohya-ss/sd-scripts (original repo, latest update)\"] {allow-input: true}\n",
        "repo_url          = repo_dict[repository]\n",
        "branch            = \"main\"  # @param {type: \"string\"}\n",
        "output_to_drive   = True  # @param {type: \"boolean\"}\n",
        "\n",
        "def clone_repo(url, dir, branch):\n",
        "    if not os.path.exists(dir):\n",
        "       !git clone -b {branch} {url} {dir}\n",
        "\n",
        "def mount_drive(dir):\n",
        "    output_dir      = os.path.join(training_dir, \"output\")\n",
        "\n",
        "    if output_to_drive:\n",
        "        if not os.path.exists(drive_dir):\n",
        "            drive.mount(os.path.dirname(drive_dir))\n",
        "        output_dir  = os.path.join(drive_dir, \"kohya-trainer/output\")\n",
        "\n",
        "    return output_dir\n",
        "\n",
        "def setup_directories():\n",
        "    global output_dir\n",
        "\n",
        "    output_dir      = mount_drive(drive_dir)\n",
        "\n",
        "    for dir in [training_dir, config_dir, pretrained_model, vae_dir, repositories_dir, output_dir]:\n",
        "        os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "def pastebin_reader(id):\n",
        "    if \"pastebin.com\" in id:\n",
        "        url = id\n",
        "        if 'raw' not in url:\n",
        "                url = url.replace('pastebin.com', 'pastebin.com/raw')\n",
        "    else:\n",
        "        url = \"https://pastebin.com/raw/\" + id\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    lines = response.text.split('\\n')\n",
        "    return lines\n",
        "\n",
        "def install_repository():\n",
        "    global infinite_image_browser_dir, voldy, discordia_archivum_dir\n",
        "\n",
        "    _, voldy = pastebin_reader(\"kq6ZmHFU\")[:2]\n",
        "\n",
        "    infinite_image_browser_url  = f\"https://github.com/zanllp/{voldy}-infinite-image-browsing.git\"\n",
        "    infinite_image_browser_dir  = os.path.join(repositories_dir, f\"infinite-image-browsing\")\n",
        "    infinite_image_browser_deps = os.path.join(infinite_image_browser_dir, \"requirements.txt\")\n",
        "\n",
        "    discordia_archivum_url = \"https://github.com/Linaqruf/discordia-archivum\"\n",
        "    discordia_archivum_dir = os.path.join(repositories_dir, \"discordia-archivum\")\n",
        "    discordia_archivum_deps = os.path.join(discordia_archivum_dir, \"requirements.txt\")\n",
        "\n",
        "    clone_repo(infinite_image_browser_url, infinite_image_browser_dir, \"main\")\n",
        "    clone_repo(discordia_archivum_url, discordia_archivum_dir, \"main\")\n",
        "\n",
        "    !pip install -q --upgrade -r {infinite_image_browser_deps}\n",
        "    !pip install python-dotenv\n",
        "    !pip install -q --upgrade -r {discordia_archivum_deps}\n",
        "\n",
        "def install_dependencies():\n",
        "    requirements_file = os.path.join(repo_dir, \"requirements.txt\")\n",
        "    model_util        = os.path.join(repo_dir, \"library/model_util.py\")\n",
        "    gpu_info          = getoutput('nvidia-smi')\n",
        "    t4_xformers_wheel = \"https://github.com/Linaqruf/colab-xformers/releases/download/0.0.20/xformers-0.0.20+1d635e1.d20230519-cp310-cp310-linux_x86_64.whl\"\n",
        "\n",
        "    !apt install aria2 lz4\n",
        "    !pip install -q --upgrade -r {requirements_file}\n",
        "\n",
        "    if '2.0.1+cu118' in torch.__version__:\n",
        "        if 'T4' in gpu_info:\n",
        "            !pip install -q {t4_xformers_wheel}\n",
        "        else:\n",
        "         !pip3 install -q --upgrade xformers==0.0.20 triton==2.0.0 --force -reinstall\n",
        "         !pip3 install diffusers==0.11.1\n",
        "         !pip3 install transformers scipy ftfy accelerate\n",
        "         !pip3 install bitsandbytes>=0.43.0\n",
        "    else:\n",
        "\n",
        "        from accelerate.utils import write_basic_config\n",
        "\n",
        "        if not os.path.exists(accelerate_config):\n",
        "          write_basic_config(save_location=accelerate_config)\n",
        "\n",
        "def prepare_environment():\n",
        "    os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "    os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n",
        "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
        "\n",
        "def main():\n",
        "    os.chdir(root_dir)\n",
        "    clone_repo(repo_url, repo_dir, branch)\n",
        "    os.chdir(repo_dir)\n",
        "    setup_directories()\n",
        "    install_repository()\n",
        "    install_dependencies()\n",
        "    prepare_environment()\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrYGu-WxFbsq",
        "outputId": "f96490b9-1c5f-4e3f-ba95-cb13833c1bdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected model: /content/pretrained_model/sd_xl_base_1.0.safetensors\n",
            "Selected vae: /content/vae/sdxl_vae.safetensors\n"
          ]
        }
      ],
      "source": [
        "# @title ## **1.2. Download SDXL**\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import glob\n",
        "import gdown\n",
        "import requests\n",
        "import subprocess\n",
        "from IPython.utils import capture\n",
        "from urllib.parse import urlparse, unquote\n",
        "from pathlib import Path\n",
        "\n",
        "%store -r\n",
        "\n",
        "os.chdir(root_dir)\n",
        "\n",
        "# @markdown Place your Huggingface [Read Token](https://huggingface.co/settings/tokens) Here. Get your SDXL access [here](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9).\n",
        "\n",
        "HUGGINGFACE_TOKEN = \"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\"#@param {type: \"string\"}\n",
        "SDXL_MODEL_URL = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors\" #@param {type: \"string\"}\n",
        "SDXL_VAE_URL = \"https://huggingface.co/stabilityai/sdxl-vae/resolve/main/sdxl_vae.safetensors\" #@param {type: \"string\"}\n",
        "\n",
        "def get_supported_extensions():\n",
        "    return tuple([\".ckpt\", \".safetensors\", \".pt\", \".pth\"])\n",
        "\n",
        "def get_filename(url, quiet=True):\n",
        "    extensions = get_supported_extensions()\n",
        "\n",
        "    if url.startswith(drive_dir) or url.endswith(tuple(extensions)):\n",
        "        filename = os.path.basename(url)\n",
        "    else:\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        if 'content-disposition' in response.headers:\n",
        "            content_disposition = response.headers['content-disposition']\n",
        "            filename = re.findall('filename=\"?([^\"]+)\"?', content_disposition)[0]\n",
        "        else:\n",
        "            url_path = urlparse(url).path\n",
        "            filename = unquote(os.path.basename(url_path))\n",
        "\n",
        "    if filename.endswith(tuple(get_supported_extensions())):\n",
        "        return filename\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def get_most_recent_file(directory):\n",
        "    files = glob.glob(os.path.join(directory, \"*\"))\n",
        "    if not files:\n",
        "        return None\n",
        "    most_recent_file = max(files, key=os.path.getmtime)\n",
        "    basename = os.path.basename(most_recent_file)\n",
        "\n",
        "    return most_recent_file\n",
        "\n",
        "def parse_args(config):\n",
        "    args = []\n",
        "\n",
        "    for k, v in config.items():\n",
        "        if k.startswith(\"_\"):\n",
        "            args.append(f\"{v}\")\n",
        "        elif isinstance(v, str) and v is not None:\n",
        "            args.append(f'--{k}={v}')\n",
        "        elif isinstance(v, bool) and v:\n",
        "            args.append(f\"--{k}\")\n",
        "        elif isinstance(v, float) and not isinstance(v, bool):\n",
        "            args.append(f\"--{k}={v}\")\n",
        "        elif isinstance(v, int) and not isinstance(v, bool):\n",
        "            args.append(f\"--{k}={v}\")\n",
        "\n",
        "    return args\n",
        "\n",
        "def aria2_download(dir, filename, url):\n",
        "    # hf_token    = \"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\" if not HUGGINGFACE_TOKEN else HUGGINGFACE_TOKEN\n",
        "    user_header = f\"Authorization: Bearer {HUGGINGFACE_TOKEN}\"\n",
        "\n",
        "    aria2_config = {\n",
        "        \"console-log-level\"         : \"error\",\n",
        "        \"summary-interval\"          : 10,\n",
        "        \"header\"                    : user_header if \"huggingface.co\" in url else None,\n",
        "        \"continue\"                  : True,\n",
        "        \"max-connection-per-server\" : 16,\n",
        "        \"min-split-size\"            : \"1M\",\n",
        "        \"split\"                     : 16,\n",
        "        \"dir\"                       : dir,\n",
        "        \"out\"                       : filename,\n",
        "        \"_url\"                      : url,\n",
        "    }\n",
        "    aria2_args = parse_args(aria2_config)\n",
        "    subprocess.run([\"aria2c\", *aria2_args])\n",
        "\n",
        "def gdown_download(url, dst, filepath):\n",
        "    if \"/uc?id/\" in url:\n",
        "        return gdown.download(url, filepath, quiet=False)\n",
        "    elif \"/file/d/\" in url:\n",
        "        return gdown.download(url=url, output=filepath, quiet=False, fuzzy=True)\n",
        "    elif \"/drive/folders/\" in url:\n",
        "        os.chdir(dst)\n",
        "        return gdown.download_folder(url, quiet=True, use_cookies=False)\n",
        "\n",
        "def download(url, dst):\n",
        "    filename = get_filename(url, quiet=False)\n",
        "    filepath = os.path.join(dst, filename)\n",
        "\n",
        "    if \"drive.google.com\" in url:\n",
        "        gdown = gdown_download(url, dst, filepath)\n",
        "    elif url.startswith(\"/content/drive/MyDrive/\"):\n",
        "        # Path(filepath).write_bytes(Path(url).read_bytes())\n",
        "        return url\n",
        "    else:\n",
        "        if \"huggingface.co\" in url:\n",
        "            if \"/blob/\" in url:\n",
        "                url = url.replace(\"/blob/\", \"/resolve/\")\n",
        "        aria2_download(dst, filename, url)\n",
        "\n",
        "def get_filepath(url, dst):\n",
        "    extensions = get_supported_extensions()\n",
        "    filename = get_filename(url)\n",
        "\n",
        "    if not filename.endswith(extensions):\n",
        "        most_recent_file = get_most_recent_file(dst)\n",
        "        filename = os.path.basename(most_recent_file)\n",
        "\n",
        "    filepath = os.path.join(dst, filename)\n",
        "\n",
        "    return filepath\n",
        "\n",
        "def main():\n",
        "    global model_path, vae_path\n",
        "\n",
        "    model_path = vae_path = None\n",
        "\n",
        "    download_targets = {\n",
        "        \"model\" : (SDXL_MODEL_URL, pretrained_model),\n",
        "        \"vae\"   : (SDXL_VAE_URL, vae_dir),\n",
        "    }\n",
        "    selected_files = {}\n",
        "\n",
        "    for target, (url, dst) in download_targets.items():\n",
        "        if url and f\"PASTE {target.upper()} URL OR GDRIVE PATH HERE\" not in url:\n",
        "            downloader = download(url, dst)\n",
        "            selected_files[target] = get_filepath(url, dst)\n",
        "\n",
        "            if target == \"model\":\n",
        "                model_path = selected_files[\"model\"] if not downloader else downloader\n",
        "            elif target == \"vae\":\n",
        "                vae_path = selected_files[\"vae\"] if not downloader else downloader\n",
        "\n",
        "    for category, path in {\n",
        "        \"model\": model_path,\n",
        "        \"vae\": vae_path,\n",
        "    }.items():\n",
        "        if path is not None and os.path.exists(path):\n",
        "            print(f\"Selected {category}: {path}\")\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kh7CeDqK4l3Y",
        "outputId": "0b2deb71-0801-493c-fa33-95cc174a81d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stored 'train_data_dir' (str)\n",
            "Your train data directory : /content/drive/MyDrive/train/js\n"
          ]
        }
      ],
      "source": [
        "# @title ## **1.3. Directory Config**\n",
        "# @markdown Specify the location of your training data in the following cell. A folder with the same name as your input will be created.\n",
        "import os\n",
        "\n",
        "%store -r\n",
        "\n",
        "train_data_dir = \"/content/drive/MyDrive/train/js\"  # @param {'type' : 'string'}\n",
        "%store train_data_dir\n",
        "\n",
        "os.makedirs(train_data_dir, exist_ok=True)\n",
        "print(f\"Your train data directory : {train_data_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show jax\n"
      ],
      "metadata": {
        "id": "hZ4B5F6mp63G",
        "outputId": "e9dad35d-a473-4944-eaa1-335bca3fcfab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: jax\n",
            "Version: 0.4.26\n",
            "Summary: Differentiate, compile, and transform Numpy code.\n",
            "Home-page: https://github.com/google/jax\n",
            "Author: JAX team\n",
            "Author-email: jax-dev@google.com\n",
            "License: Apache-2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: ml-dtypes, numpy, opt-einsum, scipy\n",
            "Required-by: chex, dopamine-rl, flax, numpyro, optax, orbax-checkpoint, scvi-tools\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhgatqF3leHJ",
        "outputId": "63140bb1-33d7-432d-be7b-540e00364edb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 58 images.\n",
            "Creating a new metadata file\n",
            "Merging tags and captions into metadata json.\n",
            "\r  0% 0/58 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "# @title ## **3.4. Bucketing and Latents Caching**\n",
        "%store -r\n",
        "\n",
        "# @markdown This code will create buckets based on the `bucket_resolution` provided for multi-aspect ratio training, and then convert all images within the `train_data_dir` to latents.\n",
        "bucketing_json    = os.path.join(training_dir, \"meta_lat.json\")\n",
        "metadata_json     = os.path.join(training_dir, \"meta_clean.json\")\n",
        "bucket_resolution = 1024  # @param {type:\"slider\", min:512, max:1024, step:128}\n",
        "mixed_precision   = \"no\"  # @param [\"no\", \"fp16\", \"bf16\"] {allow-input: false}\n",
        "flip_aug          = False  # @param{type:\"boolean\"}\n",
        "# @markdown Use `clean_caption` option to clean such as duplicate tags, `women` to `girl`, etc\n",
        "clean_caption     = True #@param {type:\"boolean\"}\n",
        "#@markdown Use the `recursive` option to process subfolders as well\n",
        "recursive         = True #@param {type:\"boolean\"}\n",
        "\n",
        "metadata_config = {\n",
        "    \"_train_data_dir\": train_data_dir,\n",
        "    \"_out_json\": metadata_json,\n",
        "    \"recursive\": recursive,\n",
        "    \"full_path\": recursive,\n",
        "    \"clean_caption\": clean_caption\n",
        "}\n",
        "\n",
        "bucketing_config = {\n",
        "    \"_train_data_dir\": train_data_dir,\n",
        "    \"_in_json\": metadata_json,\n",
        "    \"_out_json\": bucketing_json,\n",
        "    \"_model_name_or_path\": model_path,\n",
        "    \"recursive\": recursive,\n",
        "    \"full_path\": recursive,\n",
        "    \"flip_aug\": flip_aug,\n",
        "    \"batch_size\": 4,\n",
        "    \"max_data_loader_n_workers\": 2,\n",
        "    \"max_resolution\": f\"{bucket_resolution}, {bucket_resolution}\",\n",
        "    \"mixed_precision\": mixed_precision,\n",
        "}\n",
        "\n",
        "def generate_args(config):\n",
        "    args = \"\"\n",
        "    for k, v in config.items():\n",
        "        if k.startswith(\"_\"):\n",
        "            args += f'\"{v}\" '\n",
        "        elif isinstance(v, str):\n",
        "            args += f'--{k}=\"{v}\" '\n",
        "        elif isinstance(v, bool) and v:\n",
        "            args += f\"--{k} \"\n",
        "        elif isinstance(v, float) and not isinstance(v, bool):\n",
        "            args += f\"--{k}={v} \"\n",
        "        elif isinstance(v, int) and not isinstance(v, bool):\n",
        "            args += f\"--{k}={v} \"\n",
        "    return args.strip()\n",
        "\n",
        "merge_metadata_args = generate_args(metadata_config)\n",
        "prepare_buckets_args = generate_args(bucketing_config)\n",
        "\n",
        "merge_metadata_command = f\"python merge_all_to_metadata.py {merge_metadata_args}\"\n",
        "prepare_buckets_command = f\"python prepare_buckets_latents.py {prepare_buckets_args}\"\n",
        "\n",
        "os.chdir(finetune_dir)\n",
        "!{merge_metadata_command}\n",
        "time.sleep(1)\n",
        "!{prepare_buckets_command}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAZVkLuaRJ9e"
      },
      "source": [
        "# **IV. Training**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJgLfRtlHSjw",
        "outputId": "20f71ab8-2ed8-4212-d6a6-76ff4606bb00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[additional_network_arguments]\n",
            "no_metadata = false\n",
            "network_module = \"networks.lora\"\n",
            "network_dim = 32\n",
            "network_alpha = 1\n",
            "network_args = []\n",
            "network_train_unet_only = true\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import toml\n",
        "\n",
        "# @title ## **4.1. LoRa: Low-Rank Adaptation Config**\n",
        "# @markdown Kohya's `LoRA` renamed to `LoRA-LierLa` and Kohya's `LoCon` renamed to `LoRA-C3Lier`, read [official announcement](https://github.com/kohya-ss/sd-scripts/blob/849bc24d205a35fbe1b2a4063edd7172533c1c01/README.md#naming-of-lora).\n",
        "network_category = \"LoRA_LierLa\"  # @param [\"LoRA_LierLa\", \"LoRA_C3Lier\", \"DyLoRA_LierLa\", \"DyLoRA_C3Lier\", \"LoCon\", \"LoHa\", \"IA3\", \"LoKR\", \"DyLoRA_Lycoris\"]\n",
        "\n",
        "# @markdown | network_category | network_dim | network_alpha | conv_dim | conv_alpha | unit |\n",
        "# @markdown | :---: | :---: | :---: | :---: | :---: | :---: |\n",
        "# @markdown | LoRA-LierLa | 32 | 1 | - | - | - |\n",
        "# @markdown | LoCon/LoRA-C3Lier | 16 | 8 | 8 | 1 | - |\n",
        "# @markdown | LoHa | 8 | 4 | 4 | 1 | - |\n",
        "# @markdown | Other Category | ? | ? | ? | ? | - |\n",
        "\n",
        "# @markdown Specify `network_args` to add `optional` training args, like for specifying each 25 block weight, read [this](https://github.com/kohya-ss/sd-scripts/blob/main/train_network_README-ja.md#%E9%9A%8E%E5%B1%A4%E5%88%A5%E5%AD%A6%E7%BF%92%E7%8E%87)\n",
        "network_args    = \"\"  # @param {'type':'string'}\n",
        "\n",
        "# @markdown ### **Linear Layer Config**\n",
        "# @markdown Used by all `network_category`. When in doubt, set `network_dim = network_alpha`\n",
        "network_dim     = 32  # @param {'type':'number'}\n",
        "network_alpha   = 1  # @param {'type':'number'}\n",
        "\n",
        "# @markdown ### **Convolutional Layer Config**\n",
        "# @markdown Only required if `network_category` is not `LoRA_LierLa`, as it involves training convolutional layers in addition to linear layers.\n",
        "conv_dim        = 8  # @param {'type':'number'}\n",
        "conv_alpha      = 1  # @param {'type':'number'}\n",
        "\n",
        "# @markdown ### **DyLoRA Config**\n",
        "# @markdown Only required if `network_category` is `DyLoRA_LierLa` and `DyLoRA_C3Lier`\n",
        "unit = 4  # @param {'type':'number'}\n",
        "\n",
        "if isinstance(network_args, str):\n",
        "    network_args = network_args.strip()\n",
        "    if network_args.startswith('[') and network_args.endswith(']'):\n",
        "        try:\n",
        "            network_args = ast.literal_eval(network_args)\n",
        "        except (SyntaxError, ValueError) as e:\n",
        "            print(f\"Error parsing network_args: {e}\\n\")\n",
        "            network_args = []\n",
        "    elif len(network_args) > 0:\n",
        "        print(f\"WARNING! '{network_args}' is not a valid list! Put args like this: [\\\"args=1\\\", \\\"args=2\\\"]\\n\")\n",
        "        network_args = []\n",
        "    else:\n",
        "        network_args = []\n",
        "else:\n",
        "    network_args = []\n",
        "\n",
        "network_config = {\n",
        "    \"LoRA_LierLa\": {\n",
        "        \"module\": \"networks.lora\",\n",
        "        \"args\"  : []\n",
        "    },\n",
        "    \"LoRA_C3Lier\": {\n",
        "        \"module\": \"networks.lora\",\n",
        "        \"args\"  : [\n",
        "            f\"conv_dim={conv_dim}\",\n",
        "            f\"conv_alpha={conv_alpha}\"\n",
        "        ]\n",
        "    },\n",
        "    \"DyLoRA_LierLa\": {\n",
        "        \"module\": \"networks.dylora\",\n",
        "        \"args\"  : [\n",
        "            f\"unit={unit}\"\n",
        "        ]\n",
        "    },\n",
        "    \"DyLoRA_C3Lier\": {\n",
        "        \"module\": \"networks.dylora\",\n",
        "        \"args\"  : [\n",
        "            f\"conv_dim={conv_dim}\",\n",
        "            f\"conv_alpha={conv_alpha}\",\n",
        "            f\"unit={unit}\"\n",
        "        ]\n",
        "    },\n",
        "    \"LoCon\": {\n",
        "        \"module\": \"lycoris.kohya\",\n",
        "        \"args\"  : [\n",
        "            f\"algo=locon\",\n",
        "            f\"conv_dim={conv_dim}\",\n",
        "            f\"conv_alpha={conv_alpha}\"\n",
        "        ]\n",
        "    },\n",
        "    \"LoHa\": {\n",
        "        \"module\": \"lycoris.kohya\",\n",
        "        \"args\"  : [\n",
        "            f\"algo=loha\",\n",
        "            f\"conv_dim={conv_dim}\",\n",
        "            f\"conv_alpha={conv_alpha}\"\n",
        "        ]\n",
        "    },\n",
        "    \"IA3\": {\n",
        "        \"module\": \"lycoris.kohya\",\n",
        "        \"args\"  : [\n",
        "            f\"algo=ia3\",\n",
        "            f\"conv_dim={conv_dim}\",\n",
        "            f\"conv_alpha={conv_alpha}\"\n",
        "        ]\n",
        "    },\n",
        "    \"LoKR\": {\n",
        "        \"module\": \"lycoris.kohya\",\n",
        "        \"args\"  : [\n",
        "            f\"algo=lokr\",\n",
        "            f\"conv_dim={conv_dim}\",\n",
        "            f\"conv_alpha={conv_alpha}\"\n",
        "        ]\n",
        "    },\n",
        "    \"DyLoRA_Lycoris\": {\n",
        "        \"module\": \"lycoris.kohya\",\n",
        "        \"args\"  : [\n",
        "            f\"algo=dylora\",\n",
        "            f\"conv_dim={conv_dim}\",\n",
        "            f\"conv_alpha={conv_alpha}\"\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "network_module = network_config[network_category][\"module\"]\n",
        "network_args.extend(network_config[network_category][\"args\"])\n",
        "\n",
        "lora_config = {\n",
        "    \"additional_network_arguments\": {\n",
        "        \"no_metadata\"                     : False,\n",
        "        \"network_module\"                  : network_module,\n",
        "        \"network_dim\"                     : network_dim,\n",
        "        \"network_alpha\"                   : network_alpha,\n",
        "        \"network_args\"                    : network_args,\n",
        "        \"network_train_unet_only\"         : True,\n",
        "        \"training_comment\"                : None,\n",
        "    },\n",
        "}\n",
        "\n",
        "print(toml.dumps(lora_config))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNlw3u8arwir",
        "outputId": "8a6f68bc-d36f-4cd4-bdaf-2958f9c1b0cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[optimizer_arguments]\n",
            "optimizer_type = \"AdaFactor\"\n",
            "learning_rate = 0.003\n",
            "max_grad_norm = 0\n",
            "optimizer_args = [ \"scale_parameter=False\", \"relative_step=False\", \"warmup_init=False\",]\n",
            "lr_scheduler = \"adafactor\"\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import toml\n",
        "import ast\n",
        "\n",
        "# @title ## **4.2. Optimizer Config**\n",
        "# @markdown Use `Adafactor` optimizer. `RMSprop 8bit` or `Adagrad 8bit` may work. `AdamW 8bit` doesn't seem to work.\n",
        "optimizer_type = \"AdaFactor\"  # @param [\"AdamW\", \"AdamW8bit\", \"Lion8bit\", \"Lion\", \"SGDNesterov\", \"SGDNesterov8bit\", \"DAdaptation(DAdaptAdamPreprint)\", \"DAdaptAdaGrad\", \"DAdaptAdam\", \"DAdaptAdan\", \"DAdaptAdanIP\", \"DAdaptLion\", \"DAdaptSGD\", \"AdaFactor\"]\n",
        "# @markdown Specify `optimizer_args` to add `additional` args for optimizer, e.g: `[\"weight_decay=0.6\"]`\n",
        "optimizer_args = \"[ \\\"scale_parameter=False\\\", \\\"relative_step=False\\\", \\\"warmup_init=False\\\" ]\"  # @param {'type':'string'}\n",
        "# @markdown ### **Learning Rate Config**\n",
        "# @markdown Different `optimizer_type` and `network_category` for some condition requires different learning rate. It's recommended to set `text_encoder_lr = 1/2 * unet_lr`\n",
        "learning_rate = 3e-3  # @param {'type':'number'}\n",
        "# @markdown ### **LR Scheduler Config**\n",
        "# @markdown `lr_scheduler` provides several methods to adjust the learning rate based on the number of epochs.\n",
        "lr_scheduler = \"adafactor\"  # @param [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\", \"adafactor\"] {allow-input: false}\n",
        "lr_warmup_steps = None  # @param {'type':'number'}\n",
        "# @markdown Specify `lr_scheduler_num` with `num_cycles` value for `cosine_with_restarts` or `power` value for `polynomial`\n",
        "lr_scheduler_num = 1  # @param {'type':'number'}\n",
        "\n",
        "if isinstance(optimizer_args, str):\n",
        "    optimizer_args = optimizer_args.strip()\n",
        "    if optimizer_args.startswith('[') and optimizer_args.endswith(']'):\n",
        "        try:\n",
        "            optimizer_args = ast.literal_eval(optimizer_args)\n",
        "        except (SyntaxError, ValueError) as e:\n",
        "            print(f\"Error parsing optimizer_args: {e}\\n\")\n",
        "            optimizer_args = []\n",
        "    elif len(optimizer_args) > 0:\n",
        "        print(f\"WARNING! '{optimizer_args}' is not a valid list! Put args like this: [\\\"args=1\\\", \\\"args=2\\\"]\\n\")\n",
        "        optimizer_args = []\n",
        "    else:\n",
        "        optimizer_args = []\n",
        "else:\n",
        "    optimizer_args = []\n",
        "\n",
        "optimizer_config = {\n",
        "    \"optimizer_arguments\": {\n",
        "        \"optimizer_type\"          : optimizer_type,\n",
        "        \"learning_rate\"           : learning_rate,\n",
        "        \"max_grad_norm\"           : 0,\n",
        "        \"optimizer_args\"          : optimizer_args,\n",
        "        \"lr_scheduler\"            : lr_scheduler,\n",
        "        \"lr_warmup_steps\"         : lr_warmup_steps,\n",
        "        \"lr_scheduler_num_cycles\" : lr_scheduler_num if lr_scheduler == \"cosine_with_restarts\" else None,\n",
        "        \"lr_scheduler_power\"      : lr_scheduler_num if lr_scheduler == \"polynomial\" else None,\n",
        "        \"lr_scheduler_type\"       : None,\n",
        "        \"lr_scheduler_args\"       : None,\n",
        "    },\n",
        "}\n",
        "\n",
        "print(toml.dumps(optimizer_config))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOxNM0x7dvfO",
        "outputId": "2939c6b0-32be-4527-ab53-57a8040371e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[advanced_training_config]\n",
            "multires_noise_iterations = 8\n",
            "multires_noise_discount = 0.6\n",
            "min_snr_gamma = 3\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title ## **4.3. Advanced Training Config** (Optional)\n",
        "import toml\n",
        "\n",
        "# @markdown ### **Noise Control**\n",
        "noise_control_type        = \"multires_noise\" #@param [\"none\", \"noise_offset\", \"multires_noise\"]\n",
        "# @markdown #### **a. Noise Offset**\n",
        "# @markdown Control and easily generating darker or light images by offset the noise when fine-tuning the model. Recommended value: `0.1`. Read [Diffusion With Offset Noise](https://www.crosslabs.org//blog/diffusion-with-offset-noise)\n",
        "noise_offset_num          = 0.1  # @param {type:\"number\"}\n",
        "# @markdown **[Experimental]**\n",
        "# @markdown Automatically adjusts the noise offset based on the absolute mean values of each channel in the latents when used with `--noise_offset`. Specify a value around 1/10 to the same magnitude as the `--noise_offset` for best results. Set `0` to disable.\n",
        "adaptive_noise_scale      = 0.01 # @param {type:\"number\"}\n",
        "# @markdown #### **b. Multires Noise**\n",
        "# @markdown enable multires noise with this number of iterations (if enabled, around 6-10 is recommended)\n",
        "multires_noise_iterations = 8 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "multires_noise_discount = 0.6 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n",
        "# @markdown ### **Custom Train Function**\n",
        "# @markdown Gamma for reducing the weight of high-loss timesteps. Lower numbers have a stronger effect. The paper recommends `5`. Read the paper [here](https://arxiv.org/abs/2303.09556).\n",
        "min_snr_gamma             = 3 #@param {type:\"number\"}\n",
        "\n",
        "advanced_training_config = {\n",
        "    \"advanced_training_config\": {\n",
        "        \"noise_offset\"              : noise_offset_num if noise_control_type == \"noise_offset\" else None,\n",
        "        \"adaptive_noise_scale\"      : adaptive_noise_scale if adaptive_noise_scale and noise_control_type == \"noise_offset\" else None,\n",
        "        \"multires_noise_iterations\" : multires_noise_iterations if noise_control_type ==\"multires_noise\" else None,\n",
        "        \"multires_noise_discount\"   : multires_noise_discount if noise_control_type ==\"multires_noise\" else None,\n",
        "        \"min_snr_gamma\"             : min_snr_gamma if not min_snr_gamma == -1 else None,\n",
        "    }\n",
        "}\n",
        "\n",
        "print(toml.dumps(advanced_training_config))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Z4w3lfFKLjr",
        "outputId": "cc9e8abb-7296-4c23-8781-e3dc2bb5dba8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[sdxl_arguments]\n",
            "cache_text_encoder_outputs = true\n",
            "no_half_vae = true\n",
            "min_timestep = 0\n",
            "max_timestep = 1000\n",
            "shuffle_caption = false\n",
            "\n",
            "[model_arguments]\n",
            "pretrained_model_name_or_path = \"/content/pretrained_model/sd_xl_base_1.0.safetensors\"\n",
            "vae = \"/content/vae/sdxl_vae.safetensors\"\n",
            "\n",
            "[dataset_arguments]\n",
            "debug_dataset = false\n",
            "in_json = \"/content/LoRA/meta_lat.json\"\n",
            "train_data_dir = \"/content/drive/MyDrive/train/js\"\n",
            "dataset_repeats = 10\n",
            "keep_tokens = 0\n",
            "resolution = \"1024,1024\"\n",
            "color_aug = false\n",
            "token_warmup_min = 1\n",
            "token_warmup_step = 0\n",
            "\n",
            "[training_arguments]\n",
            "output_dir = \"/content/drive/MyDrive/kohya-trainer/output\"\n",
            "output_name = \"js\"\n",
            "save_precision = \"bf16\"\n",
            "save_every_n_epochs = 1\n",
            "train_batch_size = 4\n",
            "max_token_length = 225\n",
            "mem_eff_attn = false\n",
            "sdpa = false\n",
            "xformers = true\n",
            "max_train_epochs = 6\n",
            "max_data_loader_n_workers = 8\n",
            "persistent_data_loader_workers = true\n",
            "gradient_checkpointing = true\n",
            "gradient_accumulation_steps = 1\n",
            "mixed_precision = \"bf16\"\n",
            "\n",
            "[logging_arguments]\n",
            "log_with = \"wandb\"\n",
            "log_tracker_name = \"js\"\n",
            "logging_dir = \"/content/LoRA/logs\"\n",
            "\n",
            "[sample_prompt_arguments]\n",
            "sample_every_n_epochs = 1\n",
            "sample_sampler = \"euler_a\"\n",
            "\n",
            "[saving_arguments]\n",
            "save_model_as = \"safetensors\"\n",
            "\n",
            "[optimizer_arguments]\n",
            "optimizer_type = \"AdaFactor\"\n",
            "learning_rate = 0.003\n",
            "max_grad_norm = 0\n",
            "optimizer_args = [ \"scale_parameter=False\", \"relative_step=False\", \"warmup_init=False\",]\n",
            "lr_scheduler = \"adafactor\"\n",
            "\n",
            "[additional_network_arguments]\n",
            "no_metadata = false\n",
            "network_module = \"networks.lora\"\n",
            "network_dim = 32\n",
            "network_alpha = 1\n",
            "network_args = []\n",
            "network_train_unet_only = true\n",
            "\n",
            "[advanced_training_config]\n",
            "multires_noise_iterations = 8\n",
            "multires_noise_discount = 0.6\n",
            "min_snr_gamma = 3\n",
            "\n",
            "[prompt]\n",
            "width = 1024\n",
            "height = 1024\n",
            "scale = 7\n",
            "sample_steps = 28\n",
            "[[prompt.subset]]\n",
            "prompt = \"a Korean ink wash painting by Jeongseon, cityscape of Rio, \"\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title ## **4.4. Training Config**\n",
        "import toml\n",
        "import os\n",
        "from subprocess import getoutput\n",
        "\n",
        "%store -r\n",
        "\n",
        "# @markdown ### **Project Config**\n",
        "project_name                = \"js\"  # @param {type:\"string\"}\n",
        "# @markdown Get your `wandb_api_key` [here](https://wandb.ai/settings) to logs with wandb.\n",
        "wandb_api_key               = \"855a9e4f2b1aa8b2e2e27e9c1c1373071fb23c42\" # @param {type:\"string\"}\n",
        "in_json                     = \"/content/LoRA/meta_lat.json\"  # @param {type:\"string\"}\n",
        "# @markdown ### **SDXL Config**\n",
        "gradient_checkpointing      = True  # @param {type:\"boolean\"}\n",
        "no_half_vae                 = True  # @param {type:\"boolean\"}\n",
        "#@markdown Recommended parameter for SDXL training but if you enable it, `shuffle_caption` won't work\n",
        "cache_text_encoder_outputs  = True  # @param {type:\"boolean\"}\n",
        "#@markdown These options can be used to train U-Net with different timesteps. The default values are 0 and 1000.\n",
        "min_timestep                = 0 # @param {type:\"number\"}\n",
        "max_timestep                = 1000 # @param {type:\"number\"}\n",
        "# @markdown ### **Dataset Config**\n",
        "num_repeats                 = 10  # @param {type:\"number\"}\n",
        "resolution                  = 1024  # @param {type:\"slider\", min:512, max:1024, step:128}\n",
        "keep_tokens                 = 0  # @param {type:\"number\"}\n",
        "# @markdown ### **General Config**\n",
        "num_epochs                  = 6  # @param {type:\"number\"}\n",
        "train_batch_size            = 4  # @param {type:\"number\"}\n",
        "mixed_precision             = \"bf16\"  # @param [\"no\",\"fp16\",\"bf16\"] {allow-input: false}\n",
        "seed                        = -1  # @param {type:\"number\"}\n",
        "optimization                = \"xformers\" # @param [\"xformers\", \"scaled dot-product attention\"]\n",
        "# @markdown ### **Save Output Config**\n",
        "save_precision              = \"bf16\"  # @param [\"float\", \"fp16\", \"bf16\"] {allow-input: false}\n",
        "save_every_n_epochs         = 1  # @param {type:\"number\"}\n",
        "# @markdown ### **Sample Prompt Config**\n",
        "enable_sample               = True  # @param {type:\"boolean\"}\n",
        "sampler                     = \"euler_a\"  # @param [\"ddim\", \"pndm\", \"lms\", \"euler\", \"euler_a\", \"heun\", \"dpm_2\", \"dpm_2_a\", \"dpmsolver\",\"dpmsolver++\", \"dpmsingle\", \"k_lms\", \"k_euler\", \"k_euler_a\", \"k_dpm_2\", \"k_dpm_2_a\"]\n",
        "positive_prompt             = \"\"\n",
        "negative_prompt             = \"\"\n",
        "quality_prompt              = \"None\"  # @param [\"None\", \"Waifu Diffusion 1.5\", \"NovelAI\", \"AbyssOrangeMix\", \"Stable Diffusion XL\"] {allow-input: false}\n",
        "if quality_prompt          == \"Waifu Diffusion 1.5\":\n",
        "    positive_prompt         = \"(exceptional, best aesthetic, new, newest, best quality, masterpiece, extremely detailed, anime, waifu:1.2), \"\n",
        "    negative_prompt         = \"lowres, ((bad anatomy)), ((bad hands)), missing finger, extra digits, fewer digits, blurry, ((mutated hands and fingers)), (poorly drawn face), ((mutation)), ((deformed face)), (ugly), ((bad proportions)), ((extra limbs)), extra face, (double head), (extra head), ((extra feet)), monster, logo, cropped, worst quality, jpeg, humpbacked, long body, long neck, ((jpeg artifacts)), deleted, old, oldest, ((censored)), ((bad aesthetic)), (mosaic censoring, bar censor, blur censor), \"\n",
        "if quality_prompt          == \"NovelAI\":\n",
        "    positive_prompt         = \"masterpiece, best quality, \"\n",
        "    negative_prompt         = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, \"\n",
        "if quality_prompt         == \"AbyssOrangeMix\":\n",
        "    positive_prompt         = \"masterpiece, best quality, \"\n",
        "    negative_prompt         = \"(worst quality, low quality:1.4), \"\n",
        "if quality_prompt          == \"Stable Diffusion XL\":\n",
        "    negative_prompt         = \"3d render, smooth, plastic, blurry, grainy, low-resolution, deep-fried, oversaturated\"\n",
        "custom_prompt               = \"a Korean ink wash painting by Jeongseon, cityscape of Rio, \" # @param {type:\"string\"}\n",
        "# @markdown Specify `prompt_from_caption` if you want to use caption as prompt instead. Will be chosen randomly.\n",
        "prompt_from_caption         = \"none\"  # @param [\"none\", \".txt\", \".caption\"]\n",
        "if prompt_from_caption != \"none\":\n",
        "    custom_prompt           = \"\"\n",
        "num_prompt                  = 2  # @param {type:\"number\"}\n",
        "logging_dir                 = os.path.join(training_dir, \"logs\")\n",
        "\n",
        "os.chdir(repo_dir)\n",
        "\n",
        "prompt_config = {\n",
        "    \"prompt\": {\n",
        "        \"negative_prompt\" : negative_prompt,\n",
        "        \"width\"           : resolution,\n",
        "        \"height\"          : resolution,\n",
        "        \"scale\"           : 7,\n",
        "        \"sample_steps\"    : 28,\n",
        "        \"subset\"          : [],\n",
        "    }\n",
        "}\n",
        "\n",
        "train_config = {\n",
        "    \"sdxl_arguments\": {\n",
        "        \"cache_text_encoder_outputs\" : cache_text_encoder_outputs,\n",
        "        # \"enable_bucket\"              : True,\n",
        "        \"no_half_vae\"                : True,\n",
        "        # \"cache_latents\"              : True,\n",
        "        # \"cache_latents_to_disk\"      : True,\n",
        "        # \"vae_batch_size\"             : 4,\n",
        "        \"min_timestep\"               : min_timestep,\n",
        "        \"max_timestep\"               : max_timestep,\n",
        "        \"shuffle_caption\"            : True if not cache_text_encoder_outputs else False,\n",
        "    },\n",
        "    \"model_arguments\": {\n",
        "        \"pretrained_model_name_or_path\" : model_path,\n",
        "        \"vae\"                           : vae_path,\n",
        "    },\n",
        "    \"dataset_arguments\": {\n",
        "        \"debug_dataset\"                 : False,\n",
        "        \"in_json\"                       : in_json,\n",
        "        \"train_data_dir\"                : train_data_dir,\n",
        "        \"dataset_repeats\"               : num_repeats,\n",
        "        \"keep_tokens\"                   : keep_tokens,\n",
        "        \"resolution\"                    : str(resolution) + ',' + str(resolution),\n",
        "        \"color_aug\"                     : False,\n",
        "        \"face_crop_aug_range\"           : None,\n",
        "        \"token_warmup_min\"              : 1,\n",
        "        \"token_warmup_step\"             : 0,\n",
        "    },\n",
        "    \"training_arguments\": {\n",
        "        \"output_dir\"                    : output_dir,\n",
        "        \"output_name\"                   : project_name if project_name else \"last\",\n",
        "        \"save_precision\"                : save_precision,\n",
        "        \"save_every_n_epochs\"           : save_every_n_epochs,\n",
        "        \"save_n_epoch_ratio\"            : None,\n",
        "        \"save_last_n_epochs\"            : None,\n",
        "        \"save_state\"                    : None,\n",
        "        \"save_last_n_epochs_state\"      : None,\n",
        "        \"resume\"                        : None,\n",
        "        \"train_batch_size\"              : train_batch_size,\n",
        "        \"max_token_length\"              : 225,\n",
        "        \"mem_eff_attn\"                  : False,\n",
        "        \"sdpa\"                          : True if optimization == \"scaled dot-product attention\" else False,\n",
        "        \"xformers\"                      : True if optimization == \"xformers\" else False,\n",
        "        \"max_train_epochs\"              : num_epochs,\n",
        "        \"max_data_loader_n_workers\"     : 8,\n",
        "        \"persistent_data_loader_workers\": True,\n",
        "        \"seed\"                          : seed if seed > 0 else None,\n",
        "        \"gradient_checkpointing\"        : gradient_checkpointing,\n",
        "        \"gradient_accumulation_steps\"   : 1,\n",
        "        \"mixed_precision\"               : mixed_precision,\n",
        "    },\n",
        "    \"logging_arguments\": {\n",
        "        \"log_with\"          : \"wandb\" if wandb_api_key else \"tensorboard\",\n",
        "        \"log_tracker_name\"  : project_name if wandb_api_key and not project_name == \"last\" else None,\n",
        "        \"logging_dir\"       : logging_dir,\n",
        "        \"log_prefix\"        : project_name if not wandb_api_key else None,\n",
        "    },\n",
        "    \"sample_prompt_arguments\": {\n",
        "        \"sample_every_n_steps\"    : None,\n",
        "        \"sample_every_n_epochs\"   : save_every_n_epochs if enable_sample else None,\n",
        "        \"sample_sampler\"          : sampler,\n",
        "    },\n",
        "    \"saving_arguments\": {\n",
        "        \"save_model_as\": \"safetensors\"\n",
        "    },\n",
        "}\n",
        "\n",
        "def write_file(filename, contents):\n",
        "    with open(filename, \"w\") as f:\n",
        "        f.write(contents)\n",
        "\n",
        "def prompt_convert(enable_sample, num_prompt, train_data_dir, prompt_config, custom_prompt):\n",
        "    if enable_sample:\n",
        "        search_pattern = os.path.join(train_data_dir, '**/*' + prompt_from_caption)\n",
        "        caption_files = glob.glob(search_pattern, recursive=True)\n",
        "\n",
        "        if not caption_files:\n",
        "            if not custom_prompt:\n",
        "                custom_prompt = \"masterpiece, best quality, 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\"\n",
        "            new_prompt_config = prompt_config.copy()\n",
        "            new_prompt_config['prompt']['subset'] = [\n",
        "                {\"prompt\": positive_prompt + custom_prompt if positive_prompt else custom_prompt}\n",
        "            ]\n",
        "        else:\n",
        "            selected_files = random.sample(caption_files, min(num_prompt, len(caption_files)))\n",
        "\n",
        "            prompts = []\n",
        "            for file in selected_files:\n",
        "                with open(file, 'r') as f:\n",
        "                    prompts.append(f.read().strip())\n",
        "\n",
        "            new_prompt_config = prompt_config.copy()\n",
        "            new_prompt_config['prompt']['subset'] = []\n",
        "\n",
        "            for prompt in prompts:\n",
        "                new_prompt = {\n",
        "                    \"prompt\": positive_prompt + prompt if positive_prompt else prompt,\n",
        "                }\n",
        "                new_prompt_config['prompt']['subset'].append(new_prompt)\n",
        "\n",
        "        return new_prompt_config\n",
        "    else:\n",
        "        return prompt_config\n",
        "\n",
        "def eliminate_none_variable(config):\n",
        "    for key in config:\n",
        "        if isinstance(config[key], dict):\n",
        "            for sub_key in config[key]:\n",
        "                if config[key][sub_key] == \"\":\n",
        "                    config[key][sub_key] = None\n",
        "        elif config[key] == \"\":\n",
        "            config[key] = None\n",
        "\n",
        "    return config\n",
        "\n",
        "try:\n",
        "    train_config.update(optimizer_config)\n",
        "except NameError:\n",
        "    raise NameError(\"'optimizer_config' dictionary is missing. Please run  '4.1. Optimizer Config' cell.\")\n",
        "\n",
        "try:\n",
        "    train_config.update(lora_config)\n",
        "except NameError:\n",
        "    raise NameError(\"'lora_config' dictionary is missing. Please run  '4.1. LoRa: Low-Rank Adaptation Config' cell.\")\n",
        "\n",
        "advanced_training_warning = False\n",
        "try:\n",
        "    train_config.update(advanced_training_config)\n",
        "except NameError:\n",
        "    advanced_training_warning = True\n",
        "    pass\n",
        "\n",
        "prompt_config = prompt_convert(enable_sample, num_prompt, train_data_dir, prompt_config, custom_prompt)\n",
        "\n",
        "config_path         = os.path.join(config_dir, \"config_file.toml\")\n",
        "prompt_path         = os.path.join(config_dir, \"sample_prompt.toml\")\n",
        "\n",
        "config_str          = toml.dumps(eliminate_none_variable(train_config))\n",
        "prompt_str          = toml.dumps(eliminate_none_variable(prompt_config))\n",
        "\n",
        "write_file(config_path, config_str)\n",
        "write_file(prompt_path, prompt_str)\n",
        "\n",
        "print(config_str)\n",
        "\n",
        "if advanced_training_warning:\n",
        "    import textwrap\n",
        "    error_message = \"WARNING: This is not an error message, but the [advanced_training_config] dictionary is missing. Please run the '4.2. Advanced Training Config' cell if you intend to use it, or continue to the next step.\"\n",
        "    wrapped_message = textwrap.fill(error_message, width=80)\n",
        "    print('\\033[38;2;204;102;102m' + wrapped_message + '\\033[0m\\n')\n",
        "    pass\n",
        "\n",
        "print(prompt_str)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dtype"
      ],
      "metadata": {
        "id": "3qXupZzCQ-Q4",
        "outputId": "5def4aeb-c8b4-418d-a3d5-9f83df2381b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dtype\n",
            "  Downloading dtype-0.0.11-py3-none-any.whl.metadata (211 bytes)\n",
            "Downloading dtype-0.0.11-py3-none-any.whl (1.2 kB)\n",
            "Installing collected packages: dtype\n",
            "Successfully installed dtype-0.0.11\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_SHtbFwHVl1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0f42ce4-3418-4234-cb45-68d55f16c152"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.0.0+cu118 with CUDA 1108 (you have 2.2.2+cu121)\n",
            "    Python  3.10.11 (you have 3.10.12)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
            "Loading settings from /content/LoRA/config/config_file.toml...\n",
            "/content/LoRA/config/config_file\n",
            "prepare tokenizers\n",
            "update token length: 225\n",
            "Training with captions.\n",
            "loading existing metadata: /content/LoRA/meta_lat.json\n",
            "metadata has bucket info, enable bucketing / bucketbucket\n",
            "using bucket info in metadata / bucket\n",
            "[Dataset 0]\n",
            "  batch_size: 4\n",
            "  resolution: (1024, 1024)\n",
            "  enable_bucket: True\n",
            "  min_bucket_reso: None\n",
            "  max_bucket_reso: None\n",
            "  bucket_reso_steps: None\n",
            "  bucket_no_upscale: None\n",
            "\n",
            "  [Subset 0 of Dataset 0]\n",
            "    image_dir: \"/content/drive/MyDrive/train/js\"\n",
            "    image_count: 58\n",
            "    num_repeats: 10\n",
            "    shuffle_caption: False\n",
            "    keep_tokens: 0\n",
            "    caption_dropout_rate: 0.0\n",
            "    caption_dropout_every_n_epoches: 0\n",
            "    caption_tag_dropout_rate: 0.0\n",
            "    color_aug: False\n",
            "    flip_aug: False\n",
            "    face_crop_aug_range: None\n",
            "    random_crop: False\n",
            "    token_warmup_min: 1,\n",
            "    token_warmup_step: 0,\n",
            "    metadata_file: /content/LoRA/meta_lat.json\n",
            "\n",
            "\n",
            "[Dataset 0]\n",
            "loading image sizes.\n",
            "100% 58/58 [00:00<00:00, 789836.47it/s]\n",
            "make buckets\n",
            "number of images (including repeats) / bucket\n",
            "bucket 0: resolution (384, 1024), count: 10\n",
            "bucket 1: resolution (448, 1024), count: 30\n",
            "bucket 2: resolution (512, 1024), count: 10\n",
            "bucket 3: resolution (576, 1024), count: 50\n",
            "bucket 4: resolution (640, 1024), count: 10\n",
            "bucket 5: resolution (704, 1024), count: 10\n",
            "bucket 6: resolution (768, 1024), count: 240\n",
            "bucket 7: resolution (832, 1024), count: 40\n",
            "bucket 8: resolution (896, 1024), count: 10\n",
            "bucket 9: resolution (1024, 576), count: 60\n",
            "bucket 10: resolution (1024, 640), count: 10\n",
            "bucket 11: resolution (1024, 704), count: 40\n",
            "bucket 12: resolution (1024, 960), count: 20\n",
            "bucket 13: resolution (1024, 1024), count: 40\n",
            "mean ar error (without repeats): 0.0\n",
            "Warning: SDXL has been trained with noise_offset=0.0357, but noise_offset is disabled due to multires_noise_iterations / SDXLnoise_offset=0.0357multires_noise_iterationsnoise_offset\n",
            "preparing accelerator\n",
            "loading model for process 0/1\n",
            "load StableDiffusion checkpoint: /content/pretrained_model/sd_xl_base_1.0.safetensors\n",
            "building U-Net\n",
            "loading U-Net from checkpoint\n",
            "U-Net:  <All keys matched successfully>\n",
            "building text encoders\n",
            "loading text encoders from checkpoint\n",
            "text encoder 1: <All keys matched successfully>\n",
            "text encoder 2: <All keys matched successfully>\n",
            "building VAE\n",
            "loading VAE from checkpoint\n",
            "VAE: <All keys matched successfully>\n",
            "load VAE: /content/vae/sdxl_vae.safetensors\n",
            "additional VAE loaded\n",
            "Enable xformers for U-Net\n",
            "\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[2;33m/content/kohya-trainer/\u001b[0m\u001b[1;33msdxl_train_network.py\u001b[0m:\u001b[94m174\u001b[0m in \u001b[92m<module>\u001b[0m                                     \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m171 \u001b[0m\u001b[2m   \u001b[0margs = train_util.read_config_from_file(args, parser)                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m172 \u001b[0m\u001b[2m   \u001b[0m                                                                                       \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m173 \u001b[0m\u001b[2m   \u001b[0mtrainer = SdxlNetworkTrainer()                                                         \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[31m \u001b[0m174 \u001b[2m   \u001b[0mtrainer.train(args)                                                                    \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m175 \u001b[0m                                                                                           \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[2;33m/content/kohya-trainer/\u001b[0m\u001b[1;33mtrain_network.py\u001b[0m:\u001b[94m221\u001b[0m in \u001b[92mtrain\u001b[0m                                             \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m218 \u001b[0m\u001b[2m      \u001b[0m\u001b[2m#  xformers  memory efficient attention \u001b[0m                     \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m219 \u001b[0m\u001b[2m      \u001b[0mtrain_util.replace_unet_modules(unet, args.mem_eff_attn, args.xformers, args.sdp   \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m220 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m torch.__version__ >= \u001b[33m\"\u001b[0m\u001b[33m2.0.0\u001b[0m\u001b[33m\"\u001b[0m: \u001b[2m# PyTorch 2.0.0 xformers \u001b[0m   \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[31m \u001b[0m221 \u001b[2m         \u001b[0mvae.set_use_memory_efficient_attention_xformers(args.xformers)                 \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m222 \u001b[0m\u001b[2m      \u001b[0m                                                                                   \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m223 \u001b[0m\u001b[2m      \u001b[0m\u001b[2m# \u001b[0m                                             \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m224 \u001b[0m\u001b[2m      \u001b[0msys.path.append(os.path.dirname(\u001b[91m__file__\u001b[0m))                                         \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/diffusers/models/\u001b[0m\u001b[1;33mmodeling_utils.py\u001b[0m:\u001b[94m227\u001b[0m in                \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[92mset_use_memory_efficient_attention_xformers\u001b[0m                                                      \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m224 \u001b[0m\u001b[2m      \u001b[0m                                                                                   \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m225 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mfor\u001b[0m module \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.children():                                                     \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m226 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(module, torch.nn.Module):                                        \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[31m \u001b[0m227 \u001b[2m            \u001b[0mfn_recursive_set_mem_eff(module)                                           \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m228 \u001b[0m\u001b[2m   \u001b[0m                                                                                       \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m229 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92menable_xformers_memory_efficient_attention\u001b[0m(\u001b[96mself\u001b[0m, attention_op: Optional[Callable   \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m230 \u001b[0m\u001b[2;90m      \u001b[0m\u001b[33mr\u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                               \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/diffusers/models/\u001b[0m\u001b[1;33mmodeling_utils.py\u001b[0m:\u001b[94m223\u001b[0m in                \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[92mfn_recursive_set_mem_eff\u001b[0m                                                                         \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m220 \u001b[0m\u001b[2m            \u001b[0mmodule.set_use_memory_efficient_attention_xformers(valid, attention_op)    \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m221 \u001b[0m\u001b[2m         \u001b[0m                                                                               \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m222 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mfor\u001b[0m child \u001b[95min\u001b[0m module.children():                                                \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[31m \u001b[0m223 \u001b[2m            \u001b[0mfn_recursive_set_mem_eff(child)                                            \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m224 \u001b[0m\u001b[2m      \u001b[0m                                                                                   \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m225 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mfor\u001b[0m module \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.children():                                                     \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m226 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(module, torch.nn.Module):                                        \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/diffusers/models/\u001b[0m\u001b[1;33mmodeling_utils.py\u001b[0m:\u001b[94m223\u001b[0m in                \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[92mfn_recursive_set_mem_eff\u001b[0m                                                                         \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m220 \u001b[0m\u001b[2m            \u001b[0mmodule.set_use_memory_efficient_attention_xformers(valid, attention_op)    \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m221 \u001b[0m\u001b[2m         \u001b[0m                                                                               \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m222 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mfor\u001b[0m child \u001b[95min\u001b[0m module.children():                                                \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[31m \u001b[0m223 \u001b[2m            \u001b[0mfn_recursive_set_mem_eff(child)                                            \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m224 \u001b[0m\u001b[2m      \u001b[0m                                                                                   \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m225 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mfor\u001b[0m module \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.children():                                                     \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m226 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(module, torch.nn.Module):                                        \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/diffusers/models/\u001b[0m\u001b[1;33mmodeling_utils.py\u001b[0m:\u001b[94m223\u001b[0m in                \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[92mfn_recursive_set_mem_eff\u001b[0m                                                                         \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m220 \u001b[0m\u001b[2m            \u001b[0mmodule.set_use_memory_efficient_attention_xformers(valid, attention_op)    \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m221 \u001b[0m\u001b[2m         \u001b[0m                                                                               \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m222 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mfor\u001b[0m child \u001b[95min\u001b[0m module.children():                                                \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[31m \u001b[0m223 \u001b[2m            \u001b[0mfn_recursive_set_mem_eff(child)                                            \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m224 \u001b[0m\u001b[2m      \u001b[0m                                                                                   \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m225 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mfor\u001b[0m module \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.children():                                                     \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m226 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(module, torch.nn.Module):                                        \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/diffusers/models/\u001b[0m\u001b[1;33mmodeling_utils.py\u001b[0m:\u001b[94m220\u001b[0m in                \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[92mfn_recursive_set_mem_eff\u001b[0m                                                                         \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m217 \u001b[0m\u001b[2m      \u001b[0m\u001b[2m# gets the message\u001b[0m                                                                 \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m218 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mfn_recursive_set_mem_eff\u001b[0m(module: torch.nn.Module):                             \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m219 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mif\u001b[0m \u001b[96mhasattr\u001b[0m(module, \u001b[33m\"\u001b[0m\u001b[33mset_use_memory_efficient_attention_xformers\u001b[0m\u001b[33m\"\u001b[0m):             \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[31m \u001b[0m220 \u001b[2m            \u001b[0mmodule.set_use_memory_efficient_attention_xformers(valid, attention_op)    \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m221 \u001b[0m\u001b[2m         \u001b[0m                                                                               \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m222 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mfor\u001b[0m child \u001b[95min\u001b[0m module.children():                                                \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m223 \u001b[0m\u001b[2m            \u001b[0mfn_recursive_set_mem_eff(child)                                            \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/diffusers/models/\u001b[0m\u001b[1;33mattention_processor.py\u001b[0m:\u001b[94m213\u001b[0m in           \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[92mset_use_memory_efficient_attention_xformers\u001b[0m                                                      \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m 210 \u001b[0m\u001b[2m                  \u001b[0mtorch.randn((\u001b[94m1\u001b[0m, \u001b[94m2\u001b[0m, \u001b[94m40\u001b[0m), device=\u001b[33m\"\u001b[0m\u001b[33mcuda\u001b[0m\u001b[33m\"\u001b[0m),                           \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m 211 \u001b[0m\u001b[2m               \u001b[0m)                                                                     \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m 212 \u001b[0m\u001b[2m            \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mException\u001b[0m \u001b[94mas\u001b[0m e:                                                    \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 213 \u001b[2m               \u001b[0m\u001b[94mraise\u001b[0m e                                                               \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m 214 \u001b[0m\u001b[2m         \u001b[0m                                                                              \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m 215 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mif\u001b[0m is_lora:                                                                   \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m 216 \u001b[0m\u001b[2m            \u001b[0m\u001b[2m# TODO (sayakpaul): should we throw a warning if someone wants to use th\u001b[0m  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/diffusers/models/\u001b[0m\u001b[1;33mattention_processor.py\u001b[0m:\u001b[94m207\u001b[0m in           \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[92mset_use_memory_efficient_attention_xformers\u001b[0m                                                      \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m 204 \u001b[0m\u001b[2m         \u001b[0m\u001b[94melse\u001b[0m:                                                                         \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m 205 \u001b[0m\u001b[2m            \u001b[0m\u001b[94mtry\u001b[0m:                                                                      \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m 206 \u001b[0m\u001b[2m               \u001b[0m\u001b[2m# Make sure we can run the memory efficient attention\u001b[0m                 \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 207 \u001b[2m               \u001b[0m_ = xformers.ops.memory_efficient_attention(                          \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m 208 \u001b[0m\u001b[2m                  \u001b[0mtorch.randn((\u001b[94m1\u001b[0m, \u001b[94m2\u001b[0m, \u001b[94m40\u001b[0m), device=\u001b[33m\"\u001b[0m\u001b[33mcuda\u001b[0m\u001b[33m\"\u001b[0m),                           \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m 209 \u001b[0m\u001b[2m                  \u001b[0mtorch.randn((\u001b[94m1\u001b[0m, \u001b[94m2\u001b[0m, \u001b[94m40\u001b[0m), device=\u001b[33m\"\u001b[0m\u001b[33mcuda\u001b[0m\u001b[33m\"\u001b[0m),                           \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m 210 \u001b[0m\u001b[2m                  \u001b[0mtorch.randn((\u001b[94m1\u001b[0m, \u001b[94m2\u001b[0m, \u001b[94m40\u001b[0m), device=\u001b[33m\"\u001b[0m\u001b[33mcuda\u001b[0m\u001b[33m\"\u001b[0m),                           \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m192\u001b[0m in                     \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[92mmemory_efficient_attention\u001b[0m                                                                       \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m189 \u001b[0m\u001b[2;33m      \u001b[0m\u001b[33mand options.\u001b[0m                                                                       \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m190 \u001b[0m\u001b[2;33m   \u001b[0m\u001b[33m:return: multi-head attention Tensor with shape ``[B, Mq, H, Kv]``\u001b[0m                     \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m191 \u001b[0m\u001b[2;33m   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                    \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[31m \u001b[0m192 \u001b[2m   \u001b[0m\u001b[94mreturn\u001b[0m _memory_efficient_attention(                                                    \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m193 \u001b[0m\u001b[2m      \u001b[0mInputs(                                                                            \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m194 \u001b[0m\u001b[2m         \u001b[0mquery=query, key=key, value=value, p=p, attn_bias=attn_bias, scale=scale       \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m195 \u001b[0m\u001b[2m      \u001b[0m),                                                                                 \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m290\u001b[0m in                     \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[92m_memory_efficient_attention\u001b[0m                                                                      \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m287 \u001b[0m) -> torch.Tensor:                                                                         \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m288 \u001b[0m\u001b[2m   \u001b[0m\u001b[2m# fast-path that doesn't require computing the logsumexp for backward computation\u001b[0m      \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m289 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mall\u001b[0m(x.requires_grad \u001b[95mis\u001b[0m \u001b[94mFalse\u001b[0m \u001b[94mfor\u001b[0m x \u001b[95min\u001b[0m [inp.query, inp.key, inp.value]):             \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[31m \u001b[0m290 \u001b[2m      \u001b[0m\u001b[94mreturn\u001b[0m _memory_efficient_attention_forward(                                        \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m291 \u001b[0m\u001b[2m         \u001b[0minp, op=op[\u001b[94m0\u001b[0m] \u001b[94mif\u001b[0m op \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m \u001b[94mNone\u001b[0m                                      \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m292 \u001b[0m\u001b[2m      \u001b[0m)                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m293 \u001b[0m                                                                                           \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m306\u001b[0m in                     \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[92m_memory_efficient_attention_forward\u001b[0m                                                              \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m303 \u001b[0m\u001b[2m   \u001b[0minp.validate_inputs()                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m304 \u001b[0m\u001b[2m   \u001b[0moutput_shape = inp.normalize_bmhk()                                                    \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m305 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mif\u001b[0m op \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                                                         \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[31m \u001b[0m306 \u001b[2m      \u001b[0mop = _dispatch_fw(inp)                                                             \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m307 \u001b[0m\u001b[2m   \u001b[0m\u001b[94melse\u001b[0m:                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m308 \u001b[0m\u001b[2m      \u001b[0m_ensure_op_supports_or_raise(\u001b[96mValueError\u001b[0m, \u001b[33m\"\u001b[0m\u001b[33mmemory_efficient_attention\u001b[0m\u001b[33m\"\u001b[0m, op, inp)    \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m309 \u001b[0m                                                                                           \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/\u001b[0m\u001b[1;33mdispatch.py\u001b[0m:\u001b[94m104\u001b[0m in \u001b[92m_dispatch_fw\u001b[0m        \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m101 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mif\u001b[0m _is_triton_fwd_fastest(inp):                                                        \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m102 \u001b[0m\u001b[2m      \u001b[0mpriority_list_ops.remove(triton.FwOp)                                              \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m103 \u001b[0m\u001b[2m      \u001b[0mpriority_list_ops.insert(\u001b[94m0\u001b[0m, triton.FwOp)                                           \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[31m \u001b[0m104 \u001b[2m   \u001b[0m\u001b[94mreturn\u001b[0m _run_priority_list(                                                             \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m105 \u001b[0m\u001b[2m      \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mmemory_efficient_attention_forward\u001b[0m\u001b[33m\"\u001b[0m, priority_list_ops, inp                       \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m106 \u001b[0m\u001b[2m   \u001b[0m)                                                                                      \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m107 \u001b[0m                                                                                           \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/\u001b[0m\u001b[1;33mdispatch.py\u001b[0m:\u001b[94m79\u001b[0m in \u001b[92m_run_priority_list\u001b[0m   \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m 76 \u001b[0m\u001b[33m{\u001b[0mtextwrap.indent(_format_inputs_description(inp),\u001b[90m \u001b[0m\u001b[33m'\u001b[0m\u001b[33m     \u001b[0m\u001b[33m'\u001b[0m)\u001b[33m}\u001b[0m\u001b[33m\"\"\"\u001b[0m                             \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m 77 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mfor\u001b[0m op, not_supported \u001b[95min\u001b[0m \u001b[96mzip\u001b[0m(priority_list, not_supported_reasons):                    \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m 78 \u001b[0m\u001b[2m      \u001b[0mmsg += \u001b[33m\"\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m\"\u001b[0m + _format_not_supported_reasons(op, not_supported)                     \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 79 \u001b[2m   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mNotImplementedError\u001b[0m(msg)                                                         \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m 80 \u001b[0m                                                                                           \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m 81 \u001b[0m                                                                                           \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m 82 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_dispatch_fw\u001b[0m(inp: Inputs) -> Type[AttentionFwOpBase]:                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m\n",
            "\u001b[1;91mNotImplementedError: \u001b[0mNo operator found for `memory_efficient_attention_forward` with inputs:\n",
            "     query       : \u001b[33mshape\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m40\u001b[0m\u001b[1m)\u001b[0m \u001b[1m(\u001b[0mtorch.float32\u001b[1m)\u001b[0m\n",
            "     key         : \u001b[33mshape\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m40\u001b[0m\u001b[1m)\u001b[0m \u001b[1m(\u001b[0mtorch.float32\u001b[1m)\u001b[0m\n",
            "     value       : \u001b[33mshape\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m40\u001b[0m\u001b[1m)\u001b[0m \u001b[1m(\u001b[0mtorch.float32\u001b[1m)\u001b[0m\n",
            "     attn_bias   : \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'NoneType'\u001b[0m\u001b[39m>\u001b[0m\n",
            "\u001b[39m     p           : \u001b[0m\u001b[1;36m0.0\u001b[0m\n",
            "\u001b[39m`flshattF` is not supported because:\u001b[0m\n",
            "\u001b[39m    xFormers wasn't build with CUDA support\u001b[0m\n",
            "\u001b[39m    \u001b[0m\u001b[33mdtype\u001b[0m\u001b[39m=\u001b[0m\u001b[35mtorch\u001b[0m\u001b[39m.float32 \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39msupported: \u001b[0m\u001b[1;39m{\u001b[0m\u001b[39mtorch.float16, torch.bfloat16\u001b[0m\u001b[1;39m}\u001b[0m\u001b[1;39m)\u001b[0m\n",
            "\u001b[39m    Operator wasn't built - see `python -m xformers.info` for more info\u001b[0m\n",
            "\u001b[39m`tritonflashattF` is not supported because:\u001b[0m\n",
            "\u001b[39m    xFormers wasn't build with CUDA support\u001b[0m\n",
            "\u001b[39m    \u001b[0m\u001b[33mdtype\u001b[0m\u001b[39m=\u001b[0m\u001b[35mtorch\u001b[0m\u001b[39m.float32 \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39msupported: \u001b[0m\u001b[1;39m{\u001b[0m\u001b[39mtorch.float16, torch.bfloat16\u001b[0m\u001b[1;39m}\u001b[0m\u001b[1;39m)\u001b[0m\n",
            "\u001b[39m    Only work on pre-MLIR triton for now\u001b[0m\n",
            "\u001b[39m`cutlassF` is not supported because:\u001b[0m\n",
            "\u001b[39m    xFormers wasn't build with CUDA support\u001b[0m\n",
            "\u001b[39m    Operator wasn't built - see `python -m xformers.info` for more info\u001b[0m\n",
            "\u001b[39m`smallkF` is not supported because:\u001b[0m\n",
            "\u001b[39m    xFormers wasn't build with CUDA support\u001b[0m\n",
            "\u001b[39m    \u001b[0m\u001b[1;35mmax\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mquery.shape\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m-1\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m != value.shape\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m-1\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m \u001b[0m\u001b[1m>\u001b[0m \u001b[1;36m32\u001b[0m\n",
            "    Operator wasn't built - see `python -m xformers.info` for more info\n",
            "    unsupported embed per head: \u001b[1;36m40\u001b[0m\n",
            "\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/bin/\u001b[0m\u001b[1;33maccelerate\u001b[0m:\u001b[94m8\u001b[0m in \u001b[92m<module>\u001b[0m                                                          \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m5 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96maccelerate\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mcommands\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96maccelerate_cli\u001b[0m \u001b[94mimport\u001b[0m main                                          \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m6 \u001b[0m\u001b[94mif\u001b[0m \u001b[91m__name__\u001b[0m == \u001b[33m'\u001b[0m\u001b[33m__main__\u001b[0m\u001b[33m'\u001b[0m:                                                                   \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m7 \u001b[0m\u001b[2m   \u001b[0msys.argv[\u001b[94m0\u001b[0m] = re.sub(\u001b[33mr\u001b[0m\u001b[33m'\u001b[0m\u001b[33m(-script\u001b[0m\u001b[33m\\\u001b[0m\u001b[33m.pyw|\u001b[0m\u001b[33m\\\u001b[0m\u001b[33m.exe)?$\u001b[0m\u001b[33m'\u001b[0m, \u001b[33m'\u001b[0m\u001b[33m'\u001b[0m, sys.argv[\u001b[94m0\u001b[0m])                         \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[31m \u001b[0m8 \u001b[2m   \u001b[0msys.exit(main())                                                                         \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m9 \u001b[0m                                                                                             \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/accelerate/commands/\u001b[0m\u001b[1;33maccelerate_cli.py\u001b[0m:\u001b[94m45\u001b[0m in \u001b[92mmain\u001b[0m         \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m42 \u001b[0m\u001b[2m      \u001b[0mexit(\u001b[94m1\u001b[0m)                                                                             \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m43 \u001b[0m\u001b[2m   \u001b[0m                                                                                        \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m44 \u001b[0m\u001b[2m   \u001b[0m\u001b[2m# Run\u001b[0m                                                                                   \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[31m \u001b[0m45 \u001b[2m   \u001b[0margs.func(args)                                                                         \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m46 \u001b[0m                                                                                            \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m47 \u001b[0m                                                                                            \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m48 \u001b[0m\u001b[94mif\u001b[0m \u001b[91m__name__\u001b[0m == \u001b[33m\"\u001b[0m\u001b[33m__main__\u001b[0m\u001b[33m\"\u001b[0m:                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/accelerate/commands/\u001b[0m\u001b[1;33mlaunch.py\u001b[0m:\u001b[94m918\u001b[0m in \u001b[92mlaunch_command\u001b[0m      \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m915 \u001b[0m\u001b[2m   \u001b[0m\u001b[94melif\u001b[0m defaults \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[95mand\u001b[0m defaults.compute_environment == ComputeEnvironment.AMA   \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m916 \u001b[0m\u001b[2m      \u001b[0msagemaker_launcher(defaults, args)                                                 \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m917 \u001b[0m\u001b[2m   \u001b[0m\u001b[94melse\u001b[0m:                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[31m \u001b[0m918 \u001b[2m      \u001b[0msimple_launcher(args)                                                              \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m919 \u001b[0m                                                                                           \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m920 \u001b[0m                                                                                           \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m921 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mmain\u001b[0m():                                                                                \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/accelerate/commands/\u001b[0m\u001b[1;33mlaunch.py\u001b[0m:\u001b[94m580\u001b[0m in \u001b[92msimple_launcher\u001b[0m     \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m577 \u001b[0m\u001b[2m   \u001b[0mprocess.wait()                                                                         \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m578 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mif\u001b[0m process.returncode != \u001b[94m0\u001b[0m:                                                            \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m579 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m args.quiet:                                                                 \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m \u001b[31m \u001b[0m580 \u001b[2m         \u001b[0m\u001b[94mraise\u001b[0m subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)    \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m581 \u001b[0m\u001b[2m      \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m582 \u001b[0m\u001b[2m         \u001b[0msys.exit(\u001b[94m1\u001b[0m)                                                                    \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m   \u001b[2m583 \u001b[0m                                                                                           \u001b[31m\u001b[0m\n",
            "\u001b[31m\u001b[0m\n",
            "\u001b[1;91mCalledProcessError: \u001b[0mCommand \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32m'\u001b[0m\u001b[35m/usr/bin/\u001b[0m\u001b[95mpython3\u001b[0m', \u001b[32m'sdxl_train_network.py'\u001b[0m, \n",
            "\u001b[32m'--\u001b[0m\u001b[32msample_prompts\u001b[0m\u001b[32m=/content/LoRA/config/sample_prompt.toml'\u001b[0m, \n",
            "\u001b[32m'--\u001b[0m\u001b[32mconfig_file\u001b[0m\u001b[32m=/content/LoRA/config/config_file.toml'\u001b[0m\u001b[1m]\u001b[0m' returned non-zero exit status \u001b[1;36m1\u001b[0m.\n"
          ]
        }
      ],
      "source": [
        "#@title ## **4.5. Start Training**\n",
        "import os\n",
        "import toml\n",
        "\n",
        "#@markdown Check your config here if you want to edit something:\n",
        "#@markdown - `sample_prompt` : /content/LoRA/config/sample_prompt.toml\n",
        "#@markdown - `config_file` : /content/LoRA/config/config_file.toml\n",
        "\n",
        "\n",
        "#@markdown You can import config from another session if you want.\n",
        "\n",
        "sample_prompt   = \"/content/LoRA/config/sample_prompt.toml\" #@param {type:'string'}\n",
        "config_file     = \"/content/LoRA/config/config_file.toml\" #@param {type:'string'}\n",
        "\n",
        "def read_file(filename):\n",
        "    with open(filename, \"r\") as f:\n",
        "        contents = f.read()\n",
        "    return contents\n",
        "\n",
        "def train(config):\n",
        "    args = \"\"\n",
        "    for k, v in config.items():\n",
        "        if k.startswith(\"_\"):\n",
        "            args += f'\"{v}\" '\n",
        "        elif isinstance(v, str):\n",
        "            args += f'--{k}=\"{v}\" '\n",
        "        elif isinstance(v, bool) and v:\n",
        "            args += f\"--{k} \"\n",
        "        elif isinstance(v, float) and not isinstance(v, bool):\n",
        "            args += f\"--{k}={v} \"\n",
        "        elif isinstance(v, int) and not isinstance(v, bool):\n",
        "            args += f\"--{k}={v} \"\n",
        "\n",
        "    return args\n",
        "\n",
        "accelerate_conf = {\n",
        "    \"config_file\" : \"/content/kohya-trainer/accelerate_config/config.yaml\",\n",
        "    \"num_cpu_threads_per_process\" : 1,\n",
        "}\n",
        "\n",
        "train_conf = {\n",
        "    \"sample_prompts\"  : sample_prompt if os.path.exists(sample_prompt) else None,\n",
        "    \"config_file\"     : config_file,\n",
        "}\n",
        "\n",
        "accelerate_args = train(accelerate_conf)\n",
        "train_args = train(train_conf)\n",
        "\n",
        "final_args = f\"accelerate launch {accelerate_args} sdxl_train_network.py {train_args}\"\n",
        "\n",
        "os.chdir(repo_dir)\n",
        "!{final_args}"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}